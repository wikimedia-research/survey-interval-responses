---
title: "Estimation with non-linearly scaled interval responses in surveys"
description: >
  If a survey question asks about how much time the responder took to complete a task -- with the latent quantity of interest being "median task completion time" -- presenting options such as "5-15 minutes" and "1-2 days" (instead of freeform text fields) reduces cognitive load and makes analysis simpler (survey taker does not have option to omit units when answering). The proposed methodology enables accurate inference on the quantity of interest given the survey responses in the form of intervals.
author:
  - name: Demetri Pananos
    url: https://dpananos.github.io/
    affiliation: Western University
    affiliation_url: https://www.uwo.ca/
  - name: Mikhail Popov
    url: https://mpopov.com/
    affiliation: Wikimedia Foundation
    affiliation_url: https://wikimediafoundation.org/
date: "`r Sys.Date()`"
repository_url: https://github.com/wikimedia-research/survey-interval-responses
output: distill::distill_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Background

Suppose you are interested in the distribution of a continuous quantity through a survey, but you cannot record the quantity directly -- only through _bins_. For example, to protect the responder's privacy you may not want to design a question that asks them for their exact age or income, or to lessen the cognitive burden on the responder you may not want to ask them how much time they took to complete a task. To that end, you might present the responder with a set of pre-defined intervals from which they can pick the one that contains the true quantity -- thus hiding it from the survey analyst.

We are then interested in performing inference on this _latent_ quantity of interest, and specifically its distribution and the qualities of that distribution. We might be interested in the average age, the median income, or the median time to task completion -- but the data is only observed through bins.

Furthermore, those bins may not be evenly spaced or sized. For example, income is almost always right-skewed -- with more people earning less money, and fewer people earning more. Similarly, task completion times may be right-skewed as well -- with most tasks being short and quick-to-complete, and only few tasks that take a really long time. So the methodology for analyzing this binned data must enable us to infer a distribution which may be skewed or symmetrical.

In this post, we present a model and its extension -- and we demonstrate their applications through the following examples:

1. Inferring the median task completion time based on an imaginary survey in which the non-linearly scaled options were:
"0-5 minutes", "5-60 minutes", "1-6 hours", "6-12 hours", and "12-14 hours."
2. Extending the model with mixtures of densities by inferring the median incomes of two different groups of people when no other information is provided, also through non-linearly scaled options --
"0-8k", "8k-15k", "15k-30k", "30k-60k", "60k-100k", "100k-200k", "200k-500k", "More than 500k."

# Model Derivation

Data from the instruments described above are, in essence, realizations from a multinomial distribution.  Let $\mathbf{y}_i$ be the count of respondents in bin $i = 1 \dots n$, and let $N = \sum \mathbf{y}_i$ be the total number of respondents.  Then, we can model the counts in each bin as

$$ \mathbf{y} \sim \operatorname{Multinomial}(\boldsymbol{\theta}, N) $$ 

where $\boldsymbol{\theta}_i$ is the probability of observing a response in bin $i$.  We assume that the measured phenomenon (in this case, time to task completion) is distributed according to density $f$ parameterized by $\boldsymbol{\phi}$, $f_{\boldsymbol{\phi}}$.  The elements of $\boldsymbol{\theta}$ can be computed using the cumulative distribution function (CDF) of $f_{\boldsymbol{\phi}}$, $F_{\boldsymbol{\phi}}$. Let $t_i$ be bin edge $i$. Then the elements of $\boldsymbol{\theta}$ are

\begin{align}
  \boldsymbol{\theta}_1 &= F_{\boldsymbol{\phi}}(t_2)\>,   \\
  \boldsymbol{\theta}_2 &= F_{\boldsymbol{\phi}}(t_3) - F_{\boldsymbol{\phi}}( t_2) \>, \\
                        &\vdots \\
  \boldsymbol{\theta}_{n-1} &= F_{\boldsymbol{\phi}}(t_{n}) - F_{\boldsymbol{\phi}}( t_{n-1}) \>, \\
  \boldsymbol{\theta}_{n} &= 1 - F_{\boldsymbol{\phi}}( t_{n}) \>.
\end{align}

The first and last elements of $\boldsymbol{\theta}$ are constructed to soak up additional observations which may fall below/above the smallest/largest bin in the case the survey designer has included limits which do not span the support of $f_{\boldsymbol{\phi}}$. The relation between $\boldsymbol{\theta}$ and the latent density $f_{\boldsymbol{\phi}}$ allows for estimation of $\boldsymbol{\phi}$ and our latent quantity of interest "median time to task completion" using Stan.

# Setup

```{r packages}
library(tidyselect)
library(tidyverse)
library(cmdstanr)
library(gt)
library(ggplot2)
library(ggdist)
library(distributional)

register_knitr_engine(override = FALSE)
theme_set(theme_minimal(base_size = 12))
```

```{r time_scale_labeller}
time_scale_labeller <- function(x) {
  gsub("\\s0[dhms]", "", tolower(lubridate::seconds_to_period(x)))
}
# time_scale_labeller(c(3600, 3660, 3661))
```

# Example 1

This example is motivated by a real problem... Interested in the median task completion time based on an imaginary survey in which the non-linearly scaled options were: "0-5 minutes", "5-60 minutes", "1-6 hours", "6-12 hours", and "12-14 hours."

## Data Simulation

```{r samples-from-latent-dist, cache=TRUE}
set.seed(3600)
samples_from_latent_dist <- tibble(
    x1 = pmin(pmax(rlnorm(100, 8, 2), 60 * 2), 14 * 3600) # min: 2min, max: 14hr
) %>%
    mutate(
        x2 = cut(
            x1,
            breaks = c(0, 60 * 5, 3600, 3600 * 6, 3600 * 12, 3600 * 14),
            dig.lab = 5
        ),
        x3 = cut(
            x1, # in seconds
            breaks = c(0, 60 * 5, 3600, 3600 * 6, 3600 * 12, 3600 * 14),
            labels = c("(0,5]", "(5,60]", "(60,360]", "[360,720]", "(720,840]") # in minutes
        ),
        x4 = cut(
            x1, # in seconds
            breaks = c(0, 60 * 5, 3600, 3600 * 6, 3600 * 12, 3600 * 14),
            labels = c("0-5 minutes", "5-60 minutes", "1-6 hours", "6-12 hours", "12-14 hours")
        )
    )
```
```{r, layout="l-body-outset", fig.width=12, fig.height=6, fig.cap="Distribution of simulated task completion times, which will only be observed as bins."}
ggplot(samples_from_latent_dist) +
    geom_histogram(aes(x = x1), bins = 30) +
    scale_x_continuous(
        "Time spent on completing task",
        labels = function(x) time_scale_labeller(round(x)),
        breaks = 60 * c(30, 60 * 2, 60 * 6, 60 * 12),
        minor_breaks = NULL
    ) +
    labs(
        y = NULL,
        title = "Distribution of simulated task completion times"
    )
```
```{r, fig.width=12, fig.height=6, fig.cap="Simulated responses to binned survey question, by binning the hidden-to-analyst simulated task completion times."}
ggplot(samples_from_latent_dist) +
    geom_bar(aes(x = x4, y = ..count../sum(..count..))) +
    scale_y_continuous("Proportion of responses", labels = scales::percent_format(1)) +
    labs(
        x = "Time to complete task",
        title = "Simulated responses to binned survey question"
    )
```

```{r lognormal-bins, dependson='samples-from-latent-dist', cache=TRUE}
lognormal_bins <- samples_from_latent_dist %>%
    count(x2) %>%
    mutate(
      binned_samples = str_replace_all(x2, "\\[|\\]", "") %>%
        str_replace_all("\\(|\\)", '')
    ) %>% 
    separate(binned_samples, c('left', 'right'), sep = ",")
```

```{r model-data, dependson='lognormal-bins', cache=TRUE}
model_data <- list(
    n = nrow(lognormal_bins),
    edges = sort(
      as.numeric(unique(c(lognormal_bins$left, lognormal_bins$right)))
    ),
    counts = lognormal_bins$n,
    prior_mu_mean = 4.5,
    prior_mu_sigma = 2
)
```
```{r theta-true, dependson='model_data', cache=TRUE}
theta_true <- with(model_data, {
    e <- edges
    e[1] <- 0
    e[length(e)] <- Inf
    plnorm(e[2:length(edges)], 8, 2) - plnorm(e[1:length(edges)-1], 8, 2)
})
```

## Model Code

```{cmdstan multinomial-model, output.var="multinomial_model", cache=TRUE}
data {
  int<lower=1> n;
  vector[n+1] edges; // always one more edge than counts
  int counts[n];

  real prior_mu_mean;
  real prior_mu_sigma;
}
parameters {
  real mu;
  real<lower=0> sigma;
}
transformed parameters {
  simplex[n] theta;
  real exp_mu; // median

  theta[1] = lognormal_cdf(edges[2], mu, sigma);
  for (i in 2:(n-1)) {
    theta[i] = lognormal_cdf(edges[i+1], mu, sigma) - lognormal_cdf(edges[i], mu, sigma);
  }

  theta[n] = 1 - sum(theta[1:(n-1)]);
  exp_mu = exp(mu);
}
model {
    mu ~ normal(prior_mu_mean, prior_mu_sigma);
    sigma ~ cauchy(0, 5);
    counts ~ multinomial(theta);
}

```

```{r, eval=FALSE, include=FALSE}
multinomial_model <- cmdstan_model("multinomial_model_v2.stan")
```

## Results

```{r multinomial-samples, dependson=c('multinomial-model', 'model-data'), cache=TRUE, results='hide'}
multinomial_samples <- multinomial_model$sample(
  model_data, chains = 12,
  refresh = 0, show_messages = FALSE
)
```
```{r posterior-summary, dependson='multinomial-samples', cache=TRUE}
posterior_summary <- multinomial_samples$summary(
  variables = c("mu", "sigma", "theta", "exp_mu")
)
```
```{r model-draws-tidy, dependson='multinomial-samples', cache=TRUE}
model_draws_tidy <- posterior::as_draws_df(
  multinomial_samples$draws(variables = c("mu", "sigma", "theta", "exp_mu"))
)
```

```{r, echo=FALSE}
posterior_summary %>%
    mutate(ci95 = case_when(
        variable == "exp_mu" ~ sprintf("(%s, %s)", time_scale_labeller(round(q5)), time_scale_labeller(round(q95))),
        TRUE ~ sprintf("(%.2f, %.2f)", q5, q95)
    )) %>%
    left_join(tibble(
        variable = c("mu", "exp_mu", "sigma", sprintf("theta[%i]", 1:nrow(lognormal_bins))),
        truth = c(8, exp(8), 2, theta_true)
    ), by = "variable") %>%
    select(variable, truth, median, ci95) %>%
    gt(rowname_col = "variable") %>%
    fmt_number(
        columns = vars(truth, median),
        decimals = 2
    ) %>%
    fmt(
        columns = vars(truth, median),
        rows = matches("exp_mu"),
        fns = function(x) time_scale_labeller(round(x))
    )
```

```{r, layout="l-body-outset", fig.width=12, fig.height=6, fig.cap="Cumulative distribution functions of latent distribution of task completion times, based on posterior draws of the parameters. Red curve represents the true latent lognormal distribution."}
set.seed(42)
model_draws_tidy %>%
    sample_n(200) %>%
    ggplot(aes(y = 0, color = "draw")) +
    geom_vline(xintercept = exp(8), linetype = "dashed") +
    stat_dist_slab(
      aes(dist = "lnorm", arg1 = mu, arg2 = sigma),
      fill = NA, slab_type = "cdf", scale = 1
    ) +
    stat_dist_slab(
      aes(dist = "lnorm", arg1 = 8, arg2 = 2),
      fill = NA, slab_type = "cdf", color = "red", data = NULL, scale = 1
    ) +
    scale_color_manual(
      name = NULL, values = c("draw" = rgb(0, 0, 0, 0.025)), guide = FALSE
    ) +
    scale_x_log10(
        "T: time spent on completing task",
        labels = function(x) time_scale_labeller(round(x)),
        breaks = 60 * c(1, 5, 30, 60, 60 * 8, 60 * 24, 60 * 24 * 7),
        minor_breaks = NULL
    ) +
    scale_y_continuous(
        "Probability of a task completed in T time or faster",
        labels = scales::percent_format(1),
        breaks = c(0, 0.25, 0.5, 0.75, 1.0), minor_breaks = NULL
    ) +
    ggtitle(
      "Cumulative distribution of task completion times",
      "Based on posterior draws of lognormal model parameters"
    )
```

# Example 2

Suppose that in our survey of incomes, we only let responders pick the interval containing their actual income rather than asking for their exact income -- to preserve their privacy. As with the previous example our options are not evenly or linearly scaled: "0-8k", "8k-15k", "15k-30k", "30k-60k", "60k-100k", "100k-200k", "200k-500k", "More than 500k."

Furthermore, in this example we believe that there are two groups of people who make up our sample: a group which tends to earn less on average and a group which tends to earn more on average. In this imaginary scenario no additional information was collected to inform group membership, so we must infer this additional quantity.

## Model Extension

## Data Simulation

```{r group-params, cache=TRUE}
n_groups <- 200 * c(a = 0.75, b = 0.25)
group_params <- list(
  a = list(n = n_groups["a"], mu = 9, sigma = 1),
  b = list(n = n_groups["b"], mu = 11, sigma = 2)
)
```
```{r sampled-incomes, dependson='group-params'}
set.seed(401)
sampled_incomes <- group_params %>%
  map_dfr(~ tibble(income = rlnorm(.x[["n"]], .x[["mu"]], .x[["sigma"]])), .id = "group") %>%
  mutate(
    binned_income = cut(
      income,
      breaks = c(0, 8e3, 1.5e4, 3e4, 6e4, 1e5, 2e5, 5e5, Inf),
      labels = c("Less than 8k", "8k-15k", "15k-30k", "30k-60k", "60k-100k", "100k-200k", "200k-500k", "More than 500k")
    )
  )
sampled_incomes <- bind_rows(
  sampled_incomes,
  tibble(group = "both", select(sampled_incomes, income, binned_income))
)
```

```{r, echo=FALSE}
quantile_probs <- c(0, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0)
sampled_incomes %>%
  summarize(
    prob = quantile_probs,
    qs = quantile(income, prob = quantile_probs),
    .groups = "drop"
  ) %>%
  gt() %>%
  fmt_percent(columns = vars(prob), decimals = 0) %>%
  fmt_currency(columns = vars(qs)) %>%
  cols_label(prob = "Probability", qs = "Quantile") %>%
  tab_style(
    style = cell_fill(color = "gray90"),
    locations = cells_body(
      rows = (1:length(quantile_probs) %% 2) == 1
    )
  )
```

```{r income-counts, layout="l-page", echo=FALSE}
income_counts <- sampled_incomes %>%
  count(group, binned_income) %>%
  group_by(group) %>%
  mutate(prop = n / sum(n)) %>%
  ungroup %>%
  pivot_wider(names_from = "group", values_from = c("n", "prop"), values_fill = 0)
income_counts %>%
  gt(rowname_col = "binned_income") %>%
  fmt_percent(columns = starts_with("prop"), decimals = 1) %>%
  cols_label(.list = setNames(
    flatten_chr(map(c("Only A", "Only B", "Both (A & B)"), rep.int, times = 2)),
    paste0(c("n_", "prop_"), flatten_chr(map(c("a", "b", "both"), rep.int, times = 2)))
  )) %>%
  cols_align("right", starts_with("n")) %>%
  tab_spanner("Simulated responses", starts_with("n")) %>%
  tab_spanner("Proportion of total", starts_with("prop")) %>%
  cols_width(
    starts_with("n") ~ px(150),
    starts_with("prop") ~ px(150),
    everything() ~ px(150)
  ) %>%
  tab_style(
    style = cell_text(align = "right"),
    locations = cells_column_labels(everything())
  ) %>%
  tab_style(
    style = cell_fill(color = "gray90"),
    locations = cells_body(
      rows = (1:nrow(income_counts) %% 2) == 1
    )
  ) %>%
  tab_header("Binned incomes")
```

## Model Code



```{cmdstan multinomial-mixture-model, output.var="multinomial_mixture_model", cache=TRUE}
functions{
    real F_cdf(real x, real p, real mu_1, real sigma_1, real mu_2, real sigma_2){
        real mixture_1 = p*lognormal_cdf(x, mu_1, sigma_1);
        real mixture_2 = (1-p)*lognormal_cdf(x, mu_2, sigma_2);

        return mixture_1 + mixture_2;
    }
}
data {
  int<lower=1> n;
  vector[n+1] edges; // always one more edge than counts
  int counts[n];

  real prior_mu_mean;
  real prior_mu_sigma;

  real prior_p_a;
  real prior_p_b;
}
parameters {
  ordered[2] mu_k;
  real<lower=0> sigma[2];

  real<lower=0, upper=1> p;
}
transformed parameters {
  simplex[n] theta;

  theta[1] = F_cdf(edges[2], p, mu_k[1], sigma[1], mu_k[2], sigma[2]);
  for (i in 2:(n-1)) {
    theta[i] = F_cdf(edges[i+1], p, mu_k[1], sigma[1], mu_k[2], sigma[2]) - F_cdf(edges[i], p, mu_k[1], sigma[1], mu_k[2], sigma[2]);
  }

  theta[n] = 1 - F_cdf(edges[n], p, mu_k[1], sigma[1], mu_k[2], sigma[2]);
}
model {
    mu_k ~ normal(prior_mu_mean, prior_mu_sigma);
    sigma ~ cauchy(0, 5);
    p ~ beta(prior_p_a, prior_p_b);
    counts ~ multinomial(theta);
}
generated quantities{
    int yppc[size(counts)] = multinomial_rng(theta, sum(counts));
}


```

```{r multinomial-mixture-samples}
binned_incomes = sampled_incomes %>% 
                 count(binned_income)

counts = binned_incomes$n
edges = c(0, 8e3, 1.5e4, 3e4, 6e4, 1e5, 2e5, 5e5, 1e6)/1e3 # In thousands.  Last element technically inf, but make finite to pass to Stan.  Stan doesn't use it.
n = length(counts)

model_data = list(
  n = n, 
  edges = edges,
  counts = counts,
  prior_mu_mean = 0,
  prior_mu_sigma = 1,
  prior_p_a = 1,
  prior_p_b = 1
)


f = multinomial_mixture_model$sample(model_data, chains = 12, parallel_chains = 4, thin = 4)
f$cmdstan_diagnose()

r = rstan::read_stan_csv(f$output_files())

np = bayesplot::nuts_params(r)


bayesplot::mcmc_hist(r, pars = c('p'), np = np )
bayesplot::mcmc_parcoord(r, regex_pars = 'mu', np = np )
bayesplot::mcmc_parcoord(r, regex_pars = 'sigma', np = np )



```


## Results

# Extension Ideas

- Regression and hierarchical regression
- Simulation-Based Calibration (https://arxiv.org/abs/1804.06788)
- Sensitivity analysis and diagnostics
  - Model misspecification
  - Competing model selection
=======

